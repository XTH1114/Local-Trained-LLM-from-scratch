{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af90031-cfff-405c-be3e-22ed335b9f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/txu98/Desktop/CS771/LLM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_path = os.getcwd()\n",
    "print(f\"Current Working Directory: {current_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d0e55f-6031-4572-b49f-b6a34f54e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ModelArgs, Transformer\n",
    "from tokenizer import Tokenizer\n",
    "from utils import download_and_unzip\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d96149f2-2868-48b9-86a9-e9a33005130a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1bJMOyA86CDayzwmU5KjlZnbhCXHUzO41\n",
      "From (redirected): https://drive.google.com/uc?id=1bJMOyA86CDayzwmU5KjlZnbhCXHUzO41&confirm=t&uuid=d0e093c8-1518-4689-81e6-6d12d7074319\n",
      "To: /Users/txu98/Desktop/CS771/LLM/models/trained_model.pt/temp.zip\n",
      "100%|████████████████████████████████████████| 182M/182M [00:04<00:00, 41.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File extracted as: models/trained_model.pt/trained_model_tok32000.pt, saved to models/trained_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1UhsXL-ymGFy1fBftMvMbss2PGFzRxZV4\n",
      "To: /Users/txu98/Desktop/CS771/LLM/data/trained_tokenizer.model/temp.zip\n",
      "100%|████████████████████████████████████████| 500k/500k [00:00<00:00, 3.09MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File extracted as: data/trained_tokenizer.model/tok32000.model, saved to data/trained_tokenizer.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trained_model_tok32000_id = '1bJMOyA86CDayzwmU5KjlZnbhCXHUzO41'\n",
    "tok_32000_id = '1UhsXL-ymGFy1fBftMvMbss2PGFzRxZV4'\n",
    "\n",
    "checkpoint = download_and_unzip(trained_model_tok32000_id, output_dir= \"models/trained_model.pt\") # e.g., \"models/trained_model.pt\"\n",
    "tokenizer = download_and_unzip(tok_32000_id, output_dir=\"data/trained_tokenizer.model\") # e.g., \"data/trained_tokenizer.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a128e84a-ae6b-439b-9e96-7af32ebe16ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Use device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aaff6e3-9b0a-42f5-9d3c-84495b9cfe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c7/4hz617d51cx2d5h0c8ml6wqw0000gp/T/ipykernel_37815/3284602268.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_dict = torch.load(checkpoint, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dict = torch.load(checkpoint, map_location=device)\n",
    "gptconf = ModelArgs(**checkpoint_dict['model_args'])\n",
    "model = Transformer(gptconf)\n",
    "state_dict = checkpoint_dict['model']\n",
    "unwanted_prefix = '_orig_mod.' # sometimes added during compiling\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37364618-6ff6-4d2d-ac31-f29eefdfaa1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(32000, 288)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=288, out_features=288, bias=False)\n",
       "        (wk): Linear(in_features=288, out_features=288, bias=False)\n",
       "        (wv): Linear(in_features=288, out_features=288, bias=False)\n",
       "        (wo): Linear(in_features=288, out_features=288, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=288, out_features=768, bias=False)\n",
       "        (w2): Linear(in_features=768, out_features=288, bias=False)\n",
       "        (w3): Linear(in_features=288, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=288, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d50e97c0-2bb1-4aa3-8201-c729dcf16b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#words: 32000 - BOS ID: 1 - EOS ID: 2\n"
     ]
    }
   ],
   "source": [
    "enc = Tokenizer(tokenizer_model=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d624fe4d-edec-466f-9939-5b5b3d423fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "torch.backends.cuda.matmul.allow_tf32 = True #enables the use of TF32 for matrix multiplication operations within PyTorch when using CUDA\n",
    "torch.backends.cudnn.allow_tf32 = True #enables the use of TF32 precision within the cuDNN library\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a66a095-92a7-494d-bd88-4597cca88dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1 # number of samples to draw\n",
    "max_new_tokens = 256 # number of tokens generated in each sample\n",
    "temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 300 # retain only the top_k most likely tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f5a4101-0117-4116-ba2f-4d9b97037b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there is a princess called Fangqiao. He lived in a very tidy and kind heart, ready to help keep the world clear and clean. \n",
      "One day, he decided to go for a walk and ventured into his palace. As he listened to the sounds that came from the knights they surrounded him. He became a true guardian, and kept everyone safe thanks to him. \n",
      "He stayed with his kingdom and filled his heart with his comfort. He felt so comfortable and inspired by the importance of forgive. He knew that he was lucky to have such a special talent. \n",
      "At last, he returned to his palace feeling refreshed. He was pleased with himself, knowing that the compassionate owner had taught him a true moral value, and that truly appreciated is, and that is much more important than boots or cleans. Once upon a time, there was a lady who loved to count. She was very fit and could run and jump all day long. One day, she decided to count the flowers in her garden. She counted each one, one after all. She realized that counting was a great way to count things and how much her hard work turned to someone. The lady was very happy because she knew that counting is important. Once upon a\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "start = \"Once upon a time, there is a princess called Fangqiao.\"\n",
    "start_ids = enc.encode(start, bos=True, eos=False)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print(enc.decode(y[0].tolist()))\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78f1e320-d3b5-4450-afd5-3da22361b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "   @torch.inference_mode()\n",
    "   def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "       for _ in range(max_new_tokens):\n",
    "           idx_cond = idx if idx.size(1) <= self.params.max_seq_len else idx[:, -self.params.max_seq_len:]\n",
    "           logits = self(idx_cond)\n",
    "           logits = logits[:, -1, :] # crop to just the final time step\n",
    "           if temperature == 0.0:\n",
    "               _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
    "           else:\n",
    "               logits = logits / temperature\n",
    "               if top_k is not None:\n",
    "                   v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                   logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "               probs = F.softmax(logits, dim=-1)\n",
    "               idx_next = torch.multinomial(probs, num_samples=1)\n",
    "           idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "       return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0403c04-2d0e-4ca1-9ba8-f98c8922cb49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
